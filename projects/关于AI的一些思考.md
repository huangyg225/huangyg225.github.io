# 最近阅读论文的一些思考| 2026.1
最近能明显感觉到，几乎所有和 AI 相关的方向都在卷大模型。最早是文本模型，后来扩展到语音、图像、视频，甚至多模态一起上。模型越做越大，数据量也越堆越高，但真实数据已经开始碰到瓶颈，于是大家开始大量用合成数据来补。与此同时，算力门槛被拉得很高，如果没有足够的算力支持，个人基本不可能参与到真正的大模型训练里。

那问题就来了：模型越来越大、数据越来越多，这条路真的一直是对的吗？至少从目前的结果来看，答案似乎是肯定的——规模化依然能持续带来收益，还没看到明显的拐点。但如果换一个角度，从“人是怎么学习的”来思考，就会发现这条路径其实并不那么自然。人类掌握的知识量远不如大模型，更多依赖的是后天学习能力，以及对工具的使用能力。而现在的大模型，本质上还是在做大规模的知识拟合，更偏向“知道很多”，而不是“学得很快”。

具体来说，大模型在一些人类很擅长的能力上依然偏弱，比如针对特殊问题的类推能力、在新环境中的快速适应、持续学习，以及真正意义上的创新。虽然最近几年大家开始把重心转向后训练，比如指令微调、强化学习、对齐等，但一个更底层的问题仍然没有被很好地解决：有没有可能用一套相对清晰的形式化方式，去定义什么是模型的“基础能力”，什么又是“后天学习能力”？如果模型能力的提升始终依赖于不断重新训练或扩展一个更大的模型，那这种资源消耗从长远来看几乎是无穷无尽的。

在这种背景下，另一个趋势也开始变得明显：越来越多的领域不再追求一个模型包打天下，而是重新转向训练单独的特征提取器。比如，在数亿图像上专门训练一个视觉特征模型，在数亿语音数据上训练一个语音表征模型。这种方式在不少任务上是有效的，尤其是在后训练或下游任务中，相比从零开始训练，往往效率更高，也更可控。

但不管是大模型还是特征提取器，最终都会绕回到同一个老问题——数据。高质量真实数据越来越难获取，已经成了明确的瓶颈，于是合成数据被推到了台前。不过这也带来了新的挑战：怎么判断一份数据是“好”的？合成数据在多大程度上能够反映真实世界的分布？是否需要一套统一、可量化的裁判和评价标准，来衡量数据的真实性和训练价值？

期待未来AI的发展🍻
